{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da6dfec",
   "metadata": {},
   "source": [
    "# Neuroimaging data manipulation\n",
    "\n",
    "Adapted from https://github.com/miykael/workshop_pybrain\n",
    "\n",
    "The primary goal of this section is to develop a conceptual understanding of the data structures involved, to facilitate diagnosing problems in data or analysis pipelines.\n",
    "\n",
    "We'll be exploring two libraries: [nibabel](http://nipy.org/nibabel/) and [nilearn](https://nilearn.github.io/). Each of these projects has excellent documentation. While this should get you started, it is well worth your time to look through these sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ee7e8",
   "metadata": {},
   "source": [
    "## Python library `nibabel`\n",
    "<img align=\"right\" src=\"https://nipy.org/nibabel/_static/nibabel-logo.svg\" width=\"16%\">\n",
    "\n",
    "**Nibabel** is a low-level Python library that gives access to a variety of imaging formats, with a particular focus on providing a common interface to the various **volumetric** formats produced by scanners and used in common neuroimaging toolkits.\n",
    "\n",
    " - NIfTI-1\n",
    " - NIfTI-2\n",
    " - SPM Analyze\n",
    " - FreeSurfer .mgh/.mgz files\n",
    " - Philips PAR/REC\n",
    " - Siemens ECAT\n",
    " - DICOM (limited support)\n",
    "\n",
    "It also supports **surface** file formats\n",
    "\n",
    " - GIFTI\n",
    " - FreeSurfer surfaces, labels and annotations\n",
    "\n",
    "**Connectivity**\n",
    "\n",
    " - CIFTI-2\n",
    "\n",
    "**Tractography**\n",
    "\n",
    " - TrackViz .trk files\n",
    "\n",
    "And a number of related formats.\n",
    "\n",
    "**Note:** Almost all of these can be loaded through the `nibabel.load` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59beb94c",
   "metadata": {},
   "source": [
    "## Python library `nilearn`\n",
    "<img align=\"right\" src=\"https://nilearn.github.io/_static/nilearn-logo.png\" width=\"30%\">\n",
    "\n",
    "**Nilearn** labels itself as: *A Python module for fast and easy statistical learning on NeuroImaging data. It leverages the scikit-learn Python toolbox for multivariate statistics with applications such as predictive modeling, classification, decoding, or connectivity analysis.*\n",
    "\n",
    "But it's much more than that. It is also an excellent library to **manipulate** (e.g. resample images, smooth images, ROI extraction, etc.) and **visualize** your neuroimages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736eba6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "from nilearn import image as nli\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "## Set numpy to print 3 decimal points and suppress small values\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73116cbf",
   "metadata": {},
   "source": [
    "## Loading and inspecting images in `nibabel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012bffa",
   "metadata": {},
   "source": [
    "First, use the `load()` function to create a `NiBabel` image object from a NIfTI file. We’ll load in an example `T1w` and `BOLD` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5275f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_img = nib.load('sub-01_T1w.nii.gz')\n",
    "bold_img = nib.load('sub-01_task-stopmanual_run-1_bold.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab3cb7",
   "metadata": {},
   "source": [
    "Loading in a NIfTI file with `NiBabel` gives us a special type of data object which encodes all the information in the file. Each bit of information is called an attribute in Python’s terminology. To see all of these attributes, type `t1_img.` followed by pressing `Tab`. There are three main attributes that we’ll discuss today:\n",
    "* `Header`\n",
    "* `Data`\n",
    "* `Affine`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79535622",
   "metadata": {},
   "source": [
    "### Header\n",
    "`Header` contains metadata about the image, such as image dimensions, data type, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17177411",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_hdr = t1_img.header\n",
    "print(t1_hdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98a6ab",
   "metadata": {},
   "source": [
    "`t1_hdr` is a Python **dictionary**. Dictionaries are containers that hold pairs of objects - **keys** and **values**. Similar to `t1_img` in which attributes can be accessed by typing `t1_img.` followed by `Tab`, you can do the same with `t1_hdr`. We can access the value stored by a given key by typing: `t1_hdr['<key_name>']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eafe9e9",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd1e5a",
   "metadata": {},
   "source": [
    " Extract `pixdim` value from the `BOLD image` header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b838a64",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Work on your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7459f80",
   "metadata": {},
   "source": [
    "### Data\n",
    "As you’ve seen above, the header contains useful information that gives us information about the properties (metadata) associated with the MR data we’ve loaded in. Now we’ll move in to loading the actual image data itself. We can achieve this by using the method called `t1_img.get_fdata()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_data = t1_img.get_fdata()\n",
    "bold_data = bold_img.get_fdata()\n",
    "\n",
    "print(t1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521c638",
   "metadata": {},
   "source": [
    "The data is a **multidimensional array** representing the image data.\n",
    "\n",
    "How can we see the number of dimensions in the `t1_data` array? Once again, all of the attributes of the array can be seen by typing `t1_data.` followed by `Tab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions\n",
    "print('T1w image dimensions:', t1_data.ndim)\n",
    "print('BOLD image dimensions:', bold_data.ndim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how big each dimension is\n",
    "print('T1w image shape', t1_data.shape)\n",
    "print('BOLD image shape', bold_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006807d",
   "metadata": {},
   "source": [
    "The 3 numbers given here represent the number of values along a respective dimension *(x,y,z)*. For the `BOLD` image this brain was scanned in `33` axial slices with a resolution of `64 x 64` voxels per slice. That means there are:\n",
    "\n",
    "`64 * 64 * 33 = 135 168` voxels in total! And the BOLD signal was sampled `182` times. \n",
    "\n",
    "Let’s see the type of data inside of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2108d0b",
   "metadata": {},
   "source": [
    "This tells us that each element in the array (or voxel) is a floating-point number.\n",
    "The data type of an image controls the range of possible intensities. As the number of possible values increases, the amount of space the image takes up in memory also increases. \n",
    "\n",
    "Let's see what the range of this image is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('T1w image range is', str(np.min(t1_data)), 'to', str(np.max(t1_data)))\n",
    "print('BOLD image range is', str(np.min(bold_data)), 'to', str(np.max(bold_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b288d",
   "metadata": {},
   "source": [
    "\n",
    "How do we examine **what value a particular voxel is**? We can inspect the value of a voxel by selecting an index as follows:\n",
    "\n",
    "`data[x,y,z]`\n",
    "\n",
    "So for example we can inspect a voxel at coordinates `(10,20,3)` by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_data[9, 19, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff4423",
   "metadata": {},
   "source": [
    "**NOTE**: Python uses **zero-based indexing**. The first item in the array is item `0`. The second item is item `1`, the third is item `2`, etc.\n",
    "\n",
    "We can also extrect data from a **slice** (or an array) of voxels for visualisation and analysis. \n",
    "**Slicing** does exactly what it seems to imply. Giving our 3D volume, we pull out a 2D slice of our data. Below is an example of slicing from left to right (sagittal slicing, along the `x-axis`). We look at the '10th' slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1092e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slice = t1_data[9, :, :]\n",
    "print(x_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f027e9",
   "metadata": {},
   "source": [
    "This is similar to the indexing we did before to pull out a single voxel. However, instead of providing a value for each axis, the `:` indicates that we want to grab all values from that particular axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec950d",
   "metadata": {},
   "source": [
    "#### Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658dc0c",
   "metadata": {},
   "source": [
    "Now try selecting the `20th` slice from the `y axis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb32ee",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Work on your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e869dbf",
   "metadata": {},
   "source": [
    "Now try grabbing the `3rd slice` from the `z axis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a4170",
   "metadata": {},
   "source": [
    "We’ve been slicing and dicing brain images but we have no idea what they look like! Let's look how how the `100` slice of each of the `3` dimensions look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe06aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "slices = [t1_data[99, :, :], t1_data[:, 99, :], t1_data[:, :, 99]]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "for i, slice in enumerate(slices):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f060a0",
   "metadata": {},
   "source": [
    "`Nibabel` has its own viewer, which can be accessed through `img.orthoview()`.\n",
    "\n",
    "**Sidenote to plotting with orthoview()**\n",
    "\n",
    "As with other figures, if you initiated `matplotlib` with `%matplotlib inline`, the output figure will be static. If you use `orthoview()` in a normal IPython console, it will create an interactive window, and you can click to select different slices, similar to `mricron`. To get a similar experience in a `jupyter notebook`, use `%matplotlib notebook`. **But don't forget to close figures afterward again or use` %matplotlib inline` again, otherwise, you cannot plot any other figures.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "t1_img.orthoview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c48a9d",
   "metadata": {},
   "source": [
    "### Affine\n",
    "The final important piece of metadata associated with an image file is the **affine matrix**. `Affine` tells the position of the image array data in a reference space. \n",
    "\n",
    "The voxel coordinate tells us almost nothing about where the data came from in terms of position in the scanner.  For example, let’s say we have the voxel coordinate (26, 30, 16). Without more information we have no idea whether this voxel position is on the left or right of the brain, or came from the left or right of the scanner.\n",
    "\n",
    "This is because the scanner allows us to collect voxel data in almost any arbitrary position and orientation within the magnet.\n",
    "\n",
    "Usually BOLD images are acquired in a different angle and with a smaller area coverage than the T1w anatomical images - the bounding boxes are different. \n",
    "\n",
    "<img align=\"centre\" src=\"https://nipy.org/nibabel/_images/localizer.png\" width=\"70%\">\n",
    "\n",
    "The center of the BOLD image data is not quite at the center of magnet bore (the magnet *isocenter*).\n",
    "\n",
    "We have an anatomical and an BOLD scan, and later on we will surely want to be able to relate the data from subject's `_bold.nii.gz` to the same subject's `_anatomy.nii.gz`. We can’t easily do this at the moment, because we collected the anatomical image with a different field of view and orientation to the EPI image, so the voxel coordinates in the BOLD image refer to **different locations in the magnet** to the voxel coordinates in the anatomical image.\n",
    "\n",
    "We solve this problem by keeping track of the relationship of voxel coordinates to some reference space - e.g, our magnet space. The **affine array** stores the relationship between voxel coordinates in the image data array and coordinates in the reference space. Because we know the relationship of voxel coordinates to the reference space for both images, we can use this information to relate voxel coordinates in subject's `_BOLD.nii.gz` to spatially equivalent voxel coordinates in the same subject's `_anatomy.nii.gz`.\n",
    "\n",
    "The origin of the axes is at the magnet isocenter. This is coordinate `(0, 0, 0)` in our reference space. All three axes pass through the isocenter. The units of the scanner reference space are **mm**. If the subject is lying in the usual position for a brain scan, face up and head first in the scanner, then \n",
    "* scanner-left/right is also the left-**right** axis of the subject’s head, \n",
    "* scanner-floor/ceiling is the posterior-**anterior** axis of the head and \n",
    "* scanner-bore is the inferior-**superior** axis of the head.\n",
    "\n",
    "This is the most common subject-centered scanner coordinate system in neuroimaging, called **scanner RAS+** (right, anterior, superior). The **+** sign means that Right, Anterior, Superior are all positive values on these axes (and left, posterior, inferior are negative). **NOTE**: **right** to means the subject’s **right**.\n",
    "\n",
    "<img align=\"left\" src=\"https://people.cas.sc.edu/rorden/anatomy/tspace.gif\" width=\"30%\">\n",
    "\n",
    "<img align=\"right\" src=\"https://www.slicer.org/w/img_auth.php/2/22/Coordinate_sytems.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab35200",
   "metadata": {},
   "source": [
    "Below is the affine matrix for our anatomical `T1w` data. That is, relating the **voxel coordinates** to **world coordinates** in **RAS+** space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f89517",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_affine = t1_img.affine\n",
    "print(t1_affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130ccbe",
   "metadata": {},
   "source": [
    "Let's find which voxel of the `t1_img` is located at the magnet's isocenter (reference space coordinates `0 0 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, _ = np.linalg.pinv(t1_affine).dot(np.array([0, 0, 0, 1])).astype(int)\n",
    "print(\"T1w center: ({:d}, {:d}, {:d})\".format(x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94bb9f",
   "metadata": {},
   "source": [
    "Let's plot all three central slices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "slices = [t1_data[x, :, :], t1_data[:, y, :], t1_data[:, :, z]]\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "for i, slice in enumerate(slices):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4399e",
   "metadata": {},
   "source": [
    "Now, let's find and plot the center (relative to the scanner) slices of the `BOLD` image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_affine = bold_img.affine\n",
    "print(bold_affine)\n",
    "x, y, z, _ = np.linalg.pinv(bold_affine).dot(np.array([0, 0, 0, 1])).astype(int)\n",
    "print(\"BOLD center: ({:d}, {:d}, {:d})\".format(x, y, z))\n",
    "\n",
    "slices = [bold_data[x, :, :, 1], bold_data[:, y, :, 1], bold_data[:, :, z, 1]] #remember, the BOLD image is in 4-D!\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "for i, slice in enumerate(slices):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1066f6",
   "metadata": {},
   "source": [
    "How 'shifted' is the voxel space center from the reference space center?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nibabel has a function apply_affine \n",
    "from nibabel.affines import apply_affine \n",
    "\n",
    "# the central voxel in the voxel space\n",
    "bold_vox_center = (np.array(bold_data.shape) - 1) / 2.\n",
    "\n",
    "# distance from the reference space centre\n",
    "apply_affine(bold_img.affine, bold_vox_center[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8b100",
   "metadata": {},
   "source": [
    "That means the center of the image field of view is ~5.8 mm to the left from the isocenter of the magnet, and ~17.6 mm posterior to the isocenter and ~10.8 mm below (inferior) the isocenter.\n",
    "\n",
    "The parameters in the affine array can therefore give the position of any voxel coordinate, relative to the scanner RAS+ reference space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40b39f",
   "metadata": {},
   "source": [
    "We can use matrix inversion on the anatomical affine to map between voxels in the BOLD image and voxels in the anatomical image.\n",
    "\n",
    "What is the voxel coordinate in the anatomical corresponding to the voxel center of the BOLD image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as npl\n",
    "\n",
    "bold_vox2anat_vox = npl.inv(t1_img.affine).dot(bold_img.affine)\n",
    "x, y, z = apply_affine(bold_vox2anat_vox, bold_vox_center[:3])\n",
    "\n",
    "print(\"({:f}, {:f}, {:f})\".format(x, y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = [t1_data[int(x), :, :], t1_data[:, int(y), :], t1_data[:, :, int(z)]]\n",
    "fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n",
    "for i, slice in enumerate(slices):\n",
    "    axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81004269",
   "metadata": {},
   "source": [
    "How many voxels appart it is from the anatomical image voxel centre? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_vox_center = (np.array(t1_data.shape) - 1) / 2.\n",
    "[x, y, z] - t1_vox_center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1f6e7",
   "metadata": {},
   "source": [
    "When we register an image to some template, e.g., **MNI template**, we will get an affine giving the relationship between voxels in the aligned image and the MNI RAS+ space. The origin `(0, 0, 0)` ot the MNI reference space is anterior commissure (AC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ffa4c",
   "metadata": {},
   "source": [
    "## Image manipulation with `nilearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31268a1",
   "metadata": {},
   "source": [
    "### The mean image\n",
    "If you use `nibabel` to compute the mean image, you first need to load the img, get the data and then compute the mean thereof. \n",
    "\n",
    "**With `nilearn`, you can do all this in just one line with `mean image`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = nli.mean_img(bold_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = mean_img.get_fdata()\n",
    "mean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bff42",
   "metadata": {},
   "source": [
    "From version `0.5.0` on, `nilearn` provides interactive visual views. A nice alternative to `nibabel`'s `orthoview()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(mean_img, bg_img=mean_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca418b7",
   "metadata": {},
   "source": [
    "### Resample image to a template\n",
    "Using `resample_to_img`, we can resample one image to have the same dimensions as another one. For example, let's resample an anatomical `T1` image to the computed `mean` image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([mean_img.shape, t1_img.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_t1 = nli.resample_to_img(t1_img, mean_img)\n",
    "resampled_t1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4ea91",
   "metadata": {},
   "source": [
    "How does the resampled `T1` image look like? Here we will use another `nilearn` plotting function that plots a static image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50383af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_anat(t1_img, title = 'original t1', dim=-1)\n",
    "plotting.plot_anat(resampled_t1, title = 'resampled t1', dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67abab",
   "metadata": {},
   "source": [
    "### Smooth an image\n",
    "Using `smooth_img`, we can very quickly smooth any kind of MRI image. Let's, for example, take the mean image from above and smooth it with different FWHM values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da034cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fwhm in range(1, 12, 5):\n",
    "    smoothed_img = nli.smooth_img(mean_img, fwhm)\n",
    "    plotting.plot_epi(smoothed_img, title=\"Smoothing %imm\" % fwhm,\n",
    "                     display_mode='z', cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06718691",
   "metadata": {},
   "source": [
    "### Plotting a time course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f531d0",
   "metadata": {},
   "source": [
    "Let's plot a time course of the central voxel in our BOLD imgage and some other random voxel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ee6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, _ = bold_vox_center\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.transpose(bold_data[int(x), int(y), int(z), :]))\n",
    "plt.plot(np.transpose(bold_data[28, 45, 15, :]))\n",
    "plt.legend(['center voxel', 'random voxel']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac07178",
   "metadata": {},
   "source": [
    "### Masking an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7aa411",
   "metadata": {},
   "source": [
    "Thanks to `nibabel` and `nilearn` you can consider your images just a special kind of a number array. And you can do any nuber operations on the images. \n",
    "\n",
    "For example, let's take our BOLD functional image, \n",
    "1. create the mean image of it\n",
    "2. threshold it to only keep the voxels that have a value that is higher than 95% of all voxels. Of this thresholded image, we only \n",
    "3. keep those regions that are bigger than 1000mm^3. And finally, we \n",
    "4. binarize those regions to create a mask image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 create the mean image\n",
    "mean_img = nli.mean_img(bold_img)\n",
    "\n",
    "#2  keep voxels that have a value that is higher than 95% of all voxels\n",
    "thr = nli.threshold_img(mean_img, threshold='95%')\n",
    "\n",
    "#let's see how the thresholded image look compared to the original mean image\n",
    "plotting.view_img(thr, bg_img=mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 only keep those voxels that are in regions/clusters that are bigger than 1000mm^3.\n",
    "\n",
    "# get a size of 1 voxel in mm^3\n",
    "voxel_size = np.prod(thr.header['pixdim'][1:4])  \n",
    "voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the mask that only keeps those big clusters.\n",
    "from nilearn.regions import connected_regions\n",
    "\n",
    "cluster = connected_regions(thr, min_region_size=1000. / voxel_size, smoothing_fwhm=1)[0]\n",
    "\n",
    "#4 And finally, let's binarize this cluster file to create a mask.\n",
    "mask = nli.math_img('np.mean(img,axis=3) > 0', img=cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how our mask looks on the mean BOLD image\n",
    "\n",
    "from nilearn.plotting import plot_roi\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "plotting.plot_roi(mask, bg_img=mean_img, draw_cross=False, dim=-.5, cmap=ListedColormap([\"red\"]), figure=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "508.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
