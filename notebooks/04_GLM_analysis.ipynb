{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "* https://github.com/miykael/workshop_pybrain/ \n",
    "* https://nilearn.github.io/auto_examples/07_advanced/plot_bids_analysis.html\n",
    "* https://nilearn.github.io/auto_examples/07_advanced/plot_bids_analysis.html\n",
    "\n",
    "# Nilearn GLM: statistical analyses of MRI in Python\n",
    "\n",
    "\n",
    "`Nilearn`'s `GLM/stats` module allows fast and easy MRI statistical analysis.\n",
    "\n",
    "It leverages `Nibabel` and other Python libraries from the Python scientific stack like `Scipy`, `Numpy` and `Pandas`.\n",
    "\n",
    "In this tutorial, we're going to explore `nilearn's GLM` functionality by analysing a sample dataset of language localiser task. We will analyse 1) a single subject data and 2) ten subject group level example using a General Linear Model (GLM).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch example BIDS dataset\n",
    "We download a simplified `BIDS` dataset made available for illustrative\n",
    "purposes. It contains only the necessary\n",
    "information to run a statistical analysis using `Nilearn`. The raw data\n",
    "subject folders only contain `bold.json` and `events.tsv` files, while the\n",
    "`derivatives` folder includes the preprocessed files `preproc.nii` and the\n",
    "`confounds.tsv files.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_language_localizer_demo_dataset\n",
    "data_dir, _ = fetch_language_localizer_demo_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the location of the dataset on disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use, e.g., `seedir` to explore the contents of this example dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seedir as sd\n",
    "sd.seedir(data_dir, style='emoji')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM on a single subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the experimental paradigm\n",
    "We must now provide a description of the experiment, that is, define the\n",
    "timing of the task and rest periods. This is typically\n",
    "provided in an `events.tsv file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "events = pd.read_table(join(data_dir, 'sub-01/func/sub-01_task-languagelocalizer_events.tsv'))\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the GLM analysis\n",
    "It is now time to create and estimate a `FirstLevelModel` object, that will generate the *design matrix* using the  information provided by the ``events`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import FirstLevelModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of important parameters one needs to define within a `FirstLevelModel` and the majority of them will have a prominent influence on your results. Thus, make sure to check them before running your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstLevelModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = FirstLevelModel(t_r=1.5,\n",
    "                           noise_model='ar1',\n",
    "                           hrf_model='spm',\n",
    "                           drift_model='cosine',\n",
    "                           high_pass=1./160,\n",
    "                           signal_scaling=False,\n",
    "                           minimize_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we also want to include confounds computed during preprocessing (e.g., motion, global signal, etc.) as regressors of no interest. In our example, these were computed by `fmriprep` and can be found in `derivatives/fmriprep/sub-01/func/`. We can use `pandas` to inspect that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounds = pd.read_csv(join(data_dir, 'derivatives/sub-01/func/sub-01_task-languagelocalizer_desc-confounds_regressors.tsv'), \n",
    "                        delimiter='\\t')\n",
    "confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have specified the model, we can run it on the fMRI image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are doing this for the sub-01, in this example\n",
    "fmri_img = join(data_dir, 'derivatives/sub-01/func/sub-01_task-languagelocalizer_desc-preproc_bold.nii.gz')\n",
    "fmri_glm = fmri_glm.fit(fmri_img, events, confounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can inspect the design matrix (rows represent time, and columns contain the predictors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = fmri_glm.design_matrices_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we have taken the first design matrix, because the model is implictily meant to for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_design_matrix\n",
    "plot_design_matrix(design_matrix)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column contains the expected reponse profile of regions which are sensitive to the \"Finger\" task. Let's plot this first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(design_matrix['language'])\n",
    "plt.xlabel('scan')\n",
    "plt.title('Expected Response for condition \"language\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting voxels with significant effects\n",
    "\n",
    "To access the estimated coefficients (Betas of the GLM model) for each condition, we\n",
    "created constrast with a single '1' in each of the task columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "conditions = {\n",
    "    'language': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
    "    'string': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at it: plot the coefficients of the contrast, indexed by the names of the columns of the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_contrast_matrix\n",
    "plot_contrast_matrix(conditions['language'], design_matrix=design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compute the estimated effect. It is in BOLD signal unit,\n",
    "but has no statistical guarantees, because it does not take into\n",
    "account the associated variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_map = fmri_glm.compute_contrast(conditions['language'],\n",
    "                                    output_type='effect_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get statistical significance, we form a `t-statistic`, and directly convert is into `z-scale`. The `z-scale` means that the values are scaled to match a standard Gaussian distribution (mean=`0`, variance=`1`), across voxels, if there were now effects in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = fmri_glm.compute_contrast(conditions['language'],\n",
    "                                  output_type='z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot thresholded z scores map.\n",
    "\n",
    "We display it on top of the average functional image of the series (could be the anatomical image of the subject). We use arbitrarily a threshold of `3.0` in `z-scale`. We'll see later how to use corrected thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img\n",
    "mean_img = mean_img(fmri_img)\n",
    "\n",
    "from nilearn.plotting import plot_stat_map, plot_anat, plot_img, show, plot_glass_brain\n",
    "\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=3.0,\n",
    "              #display_mode='z', cut_coords=3, \n",
    "              black_bg=True,\n",
    "              title='language (Z>3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=3.0, black_bg=True, plot_abs=False,\n",
    "                 title='language (Z>3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical signifiance testing\n",
    "One should worry about the statistical validity of the procedure: here we used an arbitrary threshold of 3.0 but the threshold should provide some guarantees on the risk of false detections (aka `type-1` errors in statistics). One\n",
    "first suggestion is to **control the false positive rate** (`fpr`) at a certain level, e.g. `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "_, threshold = threshold_stats_img(z_map, alpha=.001, height_control='fpr')\n",
    "print('Uncorrected p<0.001 threshold: %.3f' % threshold)\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              #display_mode='z', cut_coords=3, \n",
    "              black_bg=True,\n",
    "              title='language (p<0.001)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='language (p<0.001)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is not corrected for **multiple comparisons**. After all, we are performing thousands of `t-tests` here (one for each voxel). A more conservative solution is to control the **family wise error** rate, i.e. the probability of making ony one false detection, say at `5%`. For that we use the so-called `Bonferroni correction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold = threshold_stats_img(z_map, alpha=.05, height_control='bonferroni')\n",
    "print('Bonferroni-corrected, p<0.05 threshold: %.3f' % threshold)\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              #display_mode='z', cut_coords=3, \n",
    "              black_bg=True,\n",
    "              title='language (p<0.05, corrected)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='language (p<0.05, corrected)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally people like to discard isolated voxels from these images. It is possible to generate a thresholded map with small clusters removed by providing a `cluster_threshold` argument. Here clusters smaller than `10` voxels will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_map, threshold = threshold_stats_img(\n",
    "    z_map, alpha=.05, height_control='bonferroni', cluster_threshold=10)\n",
    "plot_stat_map(clean_map, bg_img=mean_img, threshold=threshold,\n",
    "              #display_mode='z', cut_coords=3, \n",
    "              black_bg=True, colorbar=False,\n",
    "              title='language (p<0.05, corrected), clusters > 10 voxels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(clean_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='language (p<0.05, corrected), clusters > 10 voxels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the found positions in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import get_clusters_table\n",
    "table = get_clusters_table(z_map, stat_threshold=threshold,\n",
    "                           cluster_threshold=20)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the computed `FirstLevelModel` and contrast information, we can quickly create a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import make_glm_report\n",
    "\n",
    "report = make_glm_report(fmri_glm,\n",
    "                         contrasts='language',\n",
    "                         bg_img=mean_img\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing statistical analyses on BIDS datasets\n",
    "Even though model specification and running was comparably easy and straightforward, it can be even better. `Nilearn`'s `GLM` functionality actually enables you to define models for multiple participants through one function by leveraging the `BIDS` standard. More precisely, the function `first_level_from_bids` takes the same input arguments as `First_Level_model` (e.g. `t_r`, `hrf_model`, `high_pass`, etc.), but through defining the `BIDS raw` and `derivatives folder`, as well as a `task` and `space` label automatically extracts all information necessary to run `individual level models` and creates the `model` itself for all participants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain automatically `FirstLevelModel` objects and fit arguments\n",
    "From the dataset directory we automatically obtain the `FirstLevelModel` objects\n",
    "with their subject_id filled from the `BIDS` dataset. Moreover, we obtain\n",
    "for each model a dictionary with `run_imgs`, `events` and `confounder` regressors\n",
    "since in this case a `confounds.tsv` file is available in the `BIDS` dataset.\n",
    "To get the first level models we only have to specify the dataset directory\n",
    "and the task_label as specified in the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_from_bids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "task_label = 'languagelocalizer'\n",
    "models, models_run_imgs, models_events, models_confounds = \\\n",
    "    first_level_from_bids(\n",
    "        data_dir, task_label,\n",
    "        img_filters=[('desc', 'preproc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick sanity check on fit arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just expect one run_img per subject.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print([os.path.basename(run) for run in models_run_imgs[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only confounds stored are regressors obtained from motion correction. As\n",
    "we can verify from the column headers of the confounds table corresponding\n",
    "to the only run_img present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_confounds[0][0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this acquisition the subject read blocks of sentences and\n",
    "consonant strings. So these are our only two conditions in events.\n",
    "We verify there are 12 blocks for each condition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_events[0][0]['trial_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the `FirstLevelModel` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First level model estimation for all subjects\n",
    "Now we simply fit each first level model and plot for each subject the\n",
    "`contrast` that reveals the language network (language - string).\n",
    "Notice that we can define a contrast using the names of the conditions\n",
    "specified in the events dataframe.\n",
    "Sum, subtraction and scalar multiplication are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the threshold as the `z-variate` with an uncorrected p-value of `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "p001_unc = norm.isf(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare figure for concurrent plot of individual maps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models_fitted = [] \n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8, 4.5))\n",
    "model_and_args = zip(models, models_run_imgs, models_events, models_confounds)\n",
    "for midx, (model, imgs, events, confounds) in enumerate(model_and_args):\n",
    "    # fit the GLM\n",
    "    model.fit(imgs, events, confounds)\n",
    "    \n",
    "    models_fitted.append(model)\n",
    "    \n",
    "    # compute the contrast of interest\n",
    "    zmap = model.compute_contrast('language-string')\n",
    "    plotting.plot_glass_brain(zmap, colorbar=False, threshold=p001_unc,\n",
    "                              title=('sub-' + model.subject_label),\n",
    "                              axes=axes[int(midx / 5), int(midx % 5)],\n",
    "                              plot_abs=False, display_mode='x')\n",
    "fig.suptitle('subjects z_map language network (unc p<0.001)')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about right. However, let's also check the `design matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_design_matrix\n",
    "plot_design_matrix(models_fitted[0].design_matrices_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `contrast matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contrast_matrix('language', models_fitted[0].design_matrices_[0])\n",
    "plt.show()\n",
    "\n",
    "plot_contrast_matrix('language - string', models_fitted[0].design_matrices_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second level model estimation\n",
    "We just have to provide the list of fitted `FirstLevelModel` objects\n",
    "to the `SecondLevelModel` object for estimation. We can do this because\n",
    "all subjects share a similar design matrix (same variables reflected in\n",
    "column names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "second_level_input = models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we apply a smoothing of `8mm`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_model = SecondLevelModel(smoothing_fwhm=8.0)\n",
    "second_level_model = second_level_model.fit(second_level_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing contrasts at the second level is as simple as at the first level.\n",
    "Since we are not providing confounders we are performing a `one-sample test`\n",
    "at the second level with the images determined by the specified first level\n",
    "contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmap = second_level_model.compute_contrast(\n",
    "    first_level_contrast='language-string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group level contrast reveals a left lateralized fronto-temporal\n",
    "language network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_glass_brain(zmap, colorbar=True, threshold=p001_unc,\n",
    "                          title='Group language network (unc p<0.001)',\n",
    "                          plot_abs=False, display_mode='x')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A summary report\n",
    "And now we can create a summary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import make_glm_report\n",
    "\n",
    "report = make_glm_report(model=model,\n",
    "                         cluster_threshold = 10,\n",
    "                         contrasts='language -string',\n",
    "                         display_mode = 'ortho'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
